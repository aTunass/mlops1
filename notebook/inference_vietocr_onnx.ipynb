{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891eeb6c",
   "metadata": {},
   "source": [
    "# Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98869f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38380f76",
   "metadata": {},
   "source": [
    "# Define function and class helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f35ed68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(w, h, expected_height, image_min_width, image_max_width):\n",
    "    new_w = int(expected_height * float(w) / float(h))\n",
    "    round_to = 10\n",
    "    new_w = math.ceil(new_w/round_to)*round_to\n",
    "    new_w = max(new_w, image_min_width)\n",
    "    new_w = min(new_w, image_max_width)\n",
    "\n",
    "    return new_w, expected_height\n",
    "\n",
    "def process_image(image, image_height, image_min_width, image_max_width):\n",
    "    img = image.convert('RGB')\n",
    "    w, h = img.size\n",
    "    new_w, image_height = resize(w, h, image_height, image_min_width, image_max_width)\n",
    "    img = img.resize((new_w, image_height), Image.LANCZOS)\n",
    "    img = np.asarray(img).transpose(2,0, 1)\n",
    "    img = img/255\n",
    "    return img\n",
    "\n",
    "def process_input(image, image_height, image_min_width, image_max_width):\n",
    "    img = process_image(image, image_height, image_min_width, image_max_width)\n",
    "    img = img[np.newaxis, ...]\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, chars):\n",
    "        self.pad = 0\n",
    "        self.go = 1\n",
    "        self.eos = 2\n",
    "        self.mask_token = 3\n",
    "\n",
    "        self.chars = chars\n",
    "\n",
    "        self.c2i = {c:i+4 for i, c in enumerate(chars)}\n",
    "\n",
    "        self.i2c = {i+4:c for i, c in enumerate(chars)}\n",
    "        \n",
    "        self.i2c[0] = '<pad>'\n",
    "        self.i2c[1] = '<sos>'\n",
    "        self.i2c[2] = '<eos>'\n",
    "        self.i2c[3] = '*'\n",
    "\n",
    "    def encode(self, chars):\n",
    "        return [self.go] + [self.c2i[c] for c in chars] + [self.eos]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        first = 1 if self.go in ids else 0\n",
    "        last = ids.index(self.eos) if self.eos in ids else None\n",
    "        sent = ''.join([self.i2c[i] for i in ids[first:last]])\n",
    "        return sent\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.c2i) + 4\n",
    "    \n",
    "    def batch_decode(self, arr):\n",
    "        texts = [self.decode(ids) for ids in arr]\n",
    "        return texts\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.chars\n",
    "\n",
    "\n",
    "def translate_text(model_decoder, hidden, encoder_outputs, max_seq_length=128, sos_token=1, eos_token=2):\n",
    "\n",
    "    translated_sentence = [[sos_token]*1]\n",
    "    char_probs = [[1]*1]\n",
    "\n",
    "    max_length = 0\n",
    "    inputs = {}\n",
    "\n",
    "    while max_length <= max_seq_length and not all(np.any(np.asarray(translated_sentence).T==eos_token, axis=1)):\n",
    "        tgt_inp = np.array(translated_sentence, dtype=np.int64).T\n",
    "        \n",
    "        inputs[\"tgt_inp\"]=tgt_inp\n",
    "        inputs[\"hidden_input\"]=hidden\n",
    "        inputs[\"encoder_outputs\"]=encoder_outputs\n",
    "        output, hidden = model_decoder.run(None, inputs)\n",
    "        \n",
    "        output = np.exp(output - np.max(output, axis=-1, keepdims=True))  \n",
    "        output /= np.sum(output, axis=-1, keepdims=True)\n",
    "        \n",
    "        top_values = np.partition(output, -5, axis=-1)[:, :, -5:]  \n",
    "        top_indices = np.argsort(output, axis=-1)[:, :, -5:]  \n",
    "        \n",
    "        indices = top_indices[:, -1, -1]\n",
    "        \n",
    "        \n",
    "        values = top_values[:, -1, -1]\n",
    "        \n",
    "        char_probs.append(values.tolist())\n",
    "        translated_sentence.append(indices.tolist())   \n",
    "        max_length += 1\n",
    "\n",
    "        del output\n",
    "\n",
    "    translated_sentence = np.asarray(translated_sentence).T\n",
    "    char_probs = np.asarray(char_probs).T\n",
    "    char_probs = np.multiply(char_probs, translated_sentence>3)\n",
    "    char_probs = np.sum(char_probs, axis=-1)/(char_probs>0).sum(-1)\n",
    "    \n",
    "    return translated_sentence, char_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157fae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config/vietocr_seq2seq_config.yaml\", encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "        \n",
    "vocab = Vocab(chars=config[\"vocab\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718024f",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c09add",
   "metadata": {},
   "source": [
    "## Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "355bc226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image:  (1, 3, 32, 270) float32\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"../asset/test.png\")\n",
    "image = process_input(image,\n",
    "                        image_height=config['dataset']['image_height'], \n",
    "                        image_min_width=config['dataset']['image_min_width'], \n",
    "                        image_max_width=config['dataset']['image_max_width'])\n",
    "print(\"image: \", image.shape, image.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550cb971",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8278f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All input name: \n",
      "input_image ['batch_size', 3, 'height', 'width'] tensor(float)\n",
      "All output name: \n",
      "hidden ['batch_size', 256] tensor(float)\n",
      "encoder_outputs ['batch_size', 'src_len', 512] tensor(float)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2025-05-11 23:36:21.035857273 [E:onnxruntime:Default, provider_bridge_ort.cc:2195 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1778 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n",
      "\u001b[0;93m2025-05-11 23:36:21.035879831 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:1055 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "vietocr_encoder = ort.InferenceSession(\n",
    "        \"../checkpoint/text_recognition_encoder.onnx\",\n",
    "        providers=[('CUDAExecutionProvider')]\n",
    "    )\n",
    "print(\"All input name: \")\n",
    "for ip in vietocr_encoder.get_inputs():\n",
    "    print(ip.name, ip.shape, ip.type)\n",
    "print(\"All output name: \")\n",
    "for op in vietocr_encoder.get_outputs():\n",
    "    print(op.name, op.shape, op.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a89d226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden:  (1, 256)\n",
      "encoder_outputs:  (1, 134, 512)\n"
     ]
    }
   ],
   "source": [
    "inputs = {}\n",
    "inputs[\"input_image\"]=image\n",
    "\n",
    "hidden, encoder_outputs = vietocr_encoder.run(None, inputs)\n",
    "\n",
    "print(\"hidden: \", hidden.shape)\n",
    "print(\"encoder_outputs: \", encoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68b537",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98e6a551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All input name: \n",
      "tgt_inp ['batch_size', 'time_step'] tensor(int64)\n",
      "hidden_input ['batch_size', 256] tensor(float)\n",
      "encoder_outputs ['batch_size', 'src_len', 512] tensor(float)\n",
      "All output name: \n",
      "output ['batch_size', 1, 233] tensor(float)\n",
      "hidden_output ['batch_size', 256] tensor(float)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2025-05-11 23:38:52.384242940 [E:onnxruntime:Default, provider_bridge_ort.cc:2195 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1778 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n",
      "\u001b[0;93m2025-05-11 23:38:52.384280654 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:1055 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "vietocr_decoder = ort.InferenceSession(\n",
    "        \"../checkpoint/text_recognition_decoder.onnx\",\n",
    "        providers=[('CUDAExecutionProvider')]\n",
    "    )\n",
    "print(\"All input name: \")\n",
    "for ip in vietocr_decoder.get_inputs():\n",
    "    print(ip.name, ip.shape, ip.type)\n",
    "print(\"All output name: \")\n",
    "for op in vietocr_decoder.get_outputs():\n",
    "    print(op.name, op.shape, op.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1841f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1  97  26 144 232  76   6  98  74 232  40  12  98 232 207  77 100  26\n",
      "   42 232  98  74   6  98  76 232  98  74  76  62 232  92  78  98  76 232\n",
      "   44 100   4  98  76 208   2]]\n",
      "Text:  Mặt hàng bán (Hoặc ngành nghề kinh doanh)\n"
     ]
    }
   ],
   "source": [
    "s, prob = translate_text(model_decoder=vietocr_decoder, \n",
    "                            hidden=hidden, \n",
    "                            encoder_outputs=encoder_outputs)\n",
    "print(s)\n",
    "translated_sentence = s[0].tolist()\n",
    "text = vocab.decode(translated_sentence)\n",
    "print(\"Text: \", text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tuannha_onnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
